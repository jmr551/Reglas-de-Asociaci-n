{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Capítulo 3: El Problema de la Minería de Itemsets Frecuentes\n",
        "\n",
        "## 3.1 Definición del Problema y Complejidad Computacional\n",
        "\n",
        "### 3.1.1 Definición del Problema\n",
        "\n",
        "La **minería de itemsets frecuentes** es una tarea fundamental en la minería de datos que consiste en encontrar todos los conjuntos de ítems (itemsets) que aparecen con una frecuencia superior a un umbral mínimo en un conjunto de transacciones.\n",
        "\n",
        "#### Problema Formal\n",
        "\n",
        "Dado:\n",
        "\n",
        "- Un conjunto de ítems $ \\mathcal{I} = \\{ i_1, i_2, \\dots, i_m \\} $.\n",
        "- Un conjunto de transacciones $ \\mathcal{T} = \\{ T_1, T_2, \\dots, T_n \\} $, donde cada $ T_i \\subseteq \\mathcal{I} $.\n",
        "- Un umbral mínimo de soporte $ s_{\\text{min}} $ (expresado como una fracción o porcentaje).\n",
        "\n",
        "Objetivo:\n",
        "\n",
        "- Encontrar todos los itemsets $ X \\subseteq \\mathcal{I} $ tales que:\n",
        "\n",
        "$$ \\text{Soporte}(X) = \\frac{|\\{ T \\in \\mathcal{T} \\mid X \\subseteq T \\}|}{|\\mathcal{T}|} \\geq s_{\\text{min}} $$\n",
        "\n",
        "Estos itemsets se denominan **itemsets frecuentes**.\n",
        "\n",
        "### 3.1.2 Importancia del Problema\n",
        "\n",
        "La identificación de itemsets frecuentes es esencial porque:\n",
        "\n",
        "- Es el primer paso para generar reglas de asociación significativas.\n",
        "- Ayuda a descubrir patrones y tendencias ocultas en los datos.\n",
        "- Tiene aplicaciones en diversas áreas como análisis de mercado, bioinformática, detección de fraudes, etc.\n",
        "\n",
        "### 3.1.3 Complejidad Computacional\n",
        "\n",
        "#### Combinatorial Explosion\n",
        "\n",
        "El principal desafío en la minería de itemsets frecuentes es la **explosión combinatoria** del número de posibles itemsets. Para un conjunto de $ m $ ítems, el número total de itemsets posibles es:\n",
        "\n",
        "$$ 2^m - 1 $$\n",
        "\n",
        "Esto se debe a que cada ítem puede estar presente o no en un itemset, excepto el conjunto vacío.\n",
        "\n",
        "#### Ejemplo\n",
        "\n",
        "Si tenemos $ m = 5 $ ítems:\n",
        "\n",
        "$$ 2^5 - 1 = 31 $$ itemsets posibles.\n",
        "\n",
        "Si $ m = 20 $ ítems:\n",
        "\n",
        "$$ 2^{20} - 1 = 1,048,575 $$ itemsets posibles.\n",
        "\n",
        "A medida que $ m $ aumenta, el número de itemsets crece exponencialmente, lo que hace inviable enumerar y evaluar todos los posibles itemsets en conjuntos de datos grandes.\n",
        "\n",
        "#### Complejidad de Cálculo\n",
        "\n",
        "- **Espacio de Búsqueda Exponencial**: El número de posibles combinaciones crece exponencialmente con el número de ítems.\n",
        "- **Necesidad de Eficiencia**: Es esencial diseñar algoritmos que reduzcan el espacio de búsqueda y eviten evaluar itemsets innecesarios.\n",
        "\n",
        "## 3.2 Desafíos en Conjuntos de Datos Grandes\n",
        "\n",
        "### 3.2.1 Volumen de Datos\n",
        "\n",
        "- **Gran Número de Transacciones**: Millones de transacciones en bases de datos comerciales.\n",
        "- **Gran Número de Ítems**: Miles de ítems diferentes en catálogos de productos.\n",
        "\n",
        "### 3.2.2 Limitaciones Computacionales\n",
        "\n",
        "- **Memoria Limitada**: Los recursos de memoria son finitos y pueden agotarse al almacenar grandes estructuras de datos.\n",
        "- **Tiempo de Procesamiento**: El tiempo requerido para procesar grandes volúmenes de datos puede ser prohibitivo.\n",
        "\n",
        "### 3.2.3 Requisitos de Eficiencia\n",
        "\n",
        "- **Reducción del Espacio de Búsqueda**: Necesidad de podar itemsets infrecuentes tempranamente.\n",
        "- **Algoritmos Escalables**: Los algoritmos deben manejar eficientemente conjuntos de datos que no caben en memoria.\n",
        "- **Acceso Mínimo a Disco**: Minimizar las operaciones de lectura/escritura en disco para mejorar el rendimiento.\n",
        "\n",
        "### 3.2.4 Actualizaciones Dinámicas\n",
        "\n",
        "- **Datos Dinámicos**: Las bases de datos pueden actualizarse constantemente, requiriendo métodos que soporten actualizaciones incrementales.\n",
        "- **Necesidad de Actualizaciones Rápidas**: Recalcular itemsets frecuentes desde cero no es práctico con cada actualización.\n",
        "\n",
        "### 3.2.5 Evaluación de Resultados\n",
        "\n",
        "- **Gran Número de Itemsets Frecuentes**: Incluso después de aplicar un umbral mínimo de soporte, puede haber un número enorme de itemsets frecuentes.\n",
        "- **Relevancia de los Itemsets**: No todos los itemsets frecuentes son interesantes o útiles para el análisis.\n",
        "\n",
        "## 3.3 Análisis Teórico y Práctico con Python\n",
        "\n",
        "En esta sección, exploraremos el impacto de la explosión combinatoria y los desafíos asociados mediante ejemplos prácticos en Python.\n",
        "\n",
        "### 3.3.1 Generación de Todos los Itemsets Posibles\n",
        "\n",
        "#### Paso 1: Crear un Conjunto de Ítems\n",
        "\n",
        "Supongamos que tenemos un conjunto de \\( m = 40 \\) ítems."
      ],
      "metadata": {
        "id": "WEnxR_RGy_id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar un conjunto de 40 ítems\n",
        "items = {f'Item_{i}' for i in range(1, 41)}\n",
        "print(f\"Número de ítems: {len(items)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0zXZEFvy_X_",
        "outputId": "f7df85f9-8c47-48c1-e9b0-240d9fbfd5e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de ítems: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Salida:**\n",
        "\n",
        "```\n",
        "Número de ítems: 40\n",
        "```\n",
        "\n",
        "#### Paso 2: Calcular el Número de Posibles Itemsets"
      ],
      "metadata": {
        "id": "C7B_qlNfzPnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_itemsets = 2 ** len(items) - 1\n",
        "print(f\"Número total de itemsets posibles: {num_itemsets}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSTgJQd7zPZT",
        "outputId": "c1e74751-9347-4543-b1d1-5c1f5f9e28aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de itemsets posibles: 1099511627775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Salida:**\n",
        "\n",
        "```\n",
        "Número total de itemsets posibles: 1099511627775\n",
        "```\n",
        "\n",
        "#### Paso 3: Imposibilidad de Enumerar Todos los Itemsets\n",
        "\n",
        "Intentar generar todos los itemsets posibles para \\( m = 40 \\) ítems consume una gran cantidad de memoria y tiempo."
      ],
      "metadata": {
        "id": "cIuCkh7PzbSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "from itertools import chain, combinations\n",
        "\n",
        "def obtener_todos_los_itemsets(items):\n",
        "    # Generar todos los subconjuntos no vacíos\n",
        "    return list(chain.from_iterable(combinations(items, r) for r in range(1, len(items)+1)))\n",
        "\n",
        "# Intentar generar todos los itemsets\n",
        "#all_itemsets = obtener_todos_los_itemsets(items)\n",
        "#print(f\"Número de itemsets generados: {len(all_itemsets)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eHDE09ZzbFM",
        "outputId": "098a4826-0275-4d42-b103-4bb4e2563aea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.15 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Advertencia:** Ejecutar este código puede congelar el sistema debido al gran número de combinaciones.\n",
        "\n",
        "### 3.3.2 Impacto Práctico\n",
        "\n",
        "#### Uso de Memoria\n",
        "\n",
        "- Cada itemset requiere memoria para ser almacenado.\n",
        "- Almacenar más de un millón de itemsets puede exceder la memoria disponible.\n",
        "\n",
        "#### Tiempo de Procesamiento\n",
        "\n",
        "- El tiempo necesario para generar y procesar todos los itemsets es prohibitivo.\n",
        "- Esto demuestra la necesidad de algoritmos que eviten generar itemsets innecesarios.\n",
        "\n",
        "### 3.3.3 Necesidad de Algoritmos Eficientes\n",
        "\n",
        "Para manejar estos desafíos, se han desarrollado algoritmos como **Apriori** y **FP-Growth**, que optimizan el proceso de minería de itemsets frecuentes.\n",
        "\n",
        "- **Apriori**: Utiliza la propiedad de antimonotonía para reducir el número de itemsets candidatos.\n",
        "- **FP-Growth**: Construye una estructura compacta (árbol FP) para representar las transacciones y extraer los itemsets frecuentes sin generar candidatos.\n",
        "\n",
        "### 3.3.4 Análisis de un Algoritmo Ingenuo\n",
        "\n",
        "Para ilustrar los problemas prácticos, implementaremos un algoritmo ingenuo que intenta generar todos los itemsets posibles y calcular su soporte.\n",
        "\n",
        "#### Paso 1: Generar un Conjunto de Transacciones Simulado\n",
        "\n",
        "Crearemos un conjunto de transacciones con 100 transacciones y 40 ítems."
      ],
      "metadata": {
        "id": "FsPYDhVO4IpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Fijar la semilla para reproducibilidad\n",
        "random.seed(42)\n",
        "\n",
        "# Lista de ítems\n",
        "items = [f'Item_{i}' for i in range(1, 41)]\n",
        "\n",
        "# Generar 100 transacciones aleatorias\n",
        "transacciones = []\n",
        "for _ in range(100):\n",
        "    num_items = random.randint(1, 5)  # Cada transacción tiene entre 1 y 5 ítems\n",
        "    transaccion = set(random.sample(items, num_items))\n",
        "    transacciones.append(transaccion)"
      ],
      "metadata": {
        "id": "9oDoLiF04Ieo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 2: Implementar el Algoritmo Ingenuo\n",
        "\n",
        "Implementaremos una función que genera todos los itemsets posibles y calcula su soporte.\n",
        "\n"
      ],
      "metadata": {
        "id": "93gKzH7j4clu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "def obtener_todos_los_itemsets(transacciones):\n",
        "    items_unicos = set(chain.from_iterable(transacciones))\n",
        "    # Generar todos los itemsets posibles\n",
        "    all_itemsets = chain.from_iterable(combinations(items_unicos, r) for r in range(1, len(items_unicos)+1))\n",
        "    return list(all_itemsets)\n",
        "\n",
        "def calcular_soportes(itemsets, transacciones):\n",
        "    soporte_itemsets = {}\n",
        "    num_transacciones = len(transacciones)\n",
        "    for itemset in itemsets:\n",
        "        conteo = sum(1 for transaccion in transacciones if set(itemset).issubset(transaccion))\n",
        "        soporte_itemsets[itemset] = conteo / num_transacciones\n",
        "    return soporte_itemsets\n",
        "\n",
        "# Generar todos los itemsets posibles (¡Cuidado con el uso de memoria!)\n",
        "#itemsets = obtener_todos_los_itemsets(transacciones)\n",
        "#soportes = calcular_soportes(itemsets, transacciones)"
      ],
      "metadata": {
        "id": "8z0Pkxtc4cbN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advertencia\n",
        "\n",
        "Ejecutar este código para 40 ítems es impráctico. Incluso con 100 transacciones, el número de itemsets posibles es extremadamente grande.\n",
        "\n",
        "#### Solución: Limitar el Tamaño de los Itemsets\n",
        "\n",
        "Para fines ilustrativos, limitaremos el tamaño máximo de los itemsets a 2."
      ],
      "metadata": {
        "id": "qQKGvomn6XW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_itemsets_tamano_k(transacciones, k):\n",
        "    items_unicos = set(chain.from_iterable(transacciones))\n",
        "    # Generar itemsets de tamaño k\n",
        "    itemsets = combinations(items_unicos, k)\n",
        "    return list(itemsets)\n",
        "\n",
        "# Generar itemsets de tamaño 1 y 2\n",
        "itemsets_tamano_1 = obtener_itemsets_tamano_k(transacciones, 1)\n",
        "itemsets_tamano_2 = obtener_itemsets_tamano_k(transacciones, 2)\n",
        "\n",
        "# Unir los itemsets\n",
        "itemsets_limitados = itemsets_tamano_1 + itemsets_tamano_2\n",
        "\n",
        "# Calcular soportes\n",
        "soportes_limitados = calcular_soportes(itemsets_limitados, transacciones)\n"
      ],
      "metadata": {
        "id": "iGLQ3Hth6XL3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mostrar los Itemsets Frecuentes con Soporte Mayor a 0.1"
      ],
      "metadata": {
        "id": "v8PAWH577aIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Umbral mínimo de soporte\n",
        "min_soporte = 0.1\n",
        "\n",
        "# Filtrar itemsets frecuentes\n",
        "itemsets_frecuentes = {itemset: soporte for itemset, soporte in soportes_limitados.items() if soporte >= min_soporte}\n",
        "\n",
        "# Mostrar itemsets frecuentes\n",
        "print(\"Itemsets frecuentes (soporte >= 0.1):\")\n",
        "for itemset, soporte in itemsets_frecuentes.items():\n",
        "    print(f\"{itemset}: {soporte:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFuwLHIs7aAK",
        "outputId": "34d07467-4880-4763-a2d8-3c1d6da11124"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemsets frecuentes (soporte >= 0.1):\n",
            "('Item_39',): 0.10\n",
            "('Item_35',): 0.13\n",
            "('Item_17',): 0.13\n",
            "('Item_28',): 0.10\n",
            "('Item_7',): 0.10\n",
            "('Item_6',): 0.10\n",
            "('Item_14',): 0.12\n",
            "('Item_16',): 0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Salida (Ejemplo):**\n",
        "\n",
        "```\n",
        "Itemsets frecuentes (soporte >= 0.1):\n",
        "('Item_39',): 0.10\n",
        "('Item_35',): 0.13\n",
        "('Item_17',): 0.13\n",
        "('Item_28',): 0.10\n",
        "('Item_7',): 0.10\n",
        "('Item_6',): 0.10\n",
        "('Item_14',): 0.12\n",
        "('Item_16',): 0.12\n",
        "```\n",
        "\n",
        "### 3.3.5 Limitaciones del Algoritmo Ingenuo\n",
        "\n",
        "- **No Escala Bien**: Incluso con limitaciones, el algoritmo se vuelve ineficiente con más ítems o transacciones.\n",
        "- **No Aprovecha Propiedades**: No utiliza propiedades como la antimonotonía para reducir el espacio de búsqueda.\n",
        "- **Impráctico para Datos Reales**: No es viable para conjuntos de datos reales con miles de ítems y millones de transacciones.\n",
        "\n",
        "## 3.4 Quiz\n",
        "\n",
        "**Pregunta 1:** Explique por qué el número de posibles itemsets en un conjunto de ítems es $ 2^m - 1 $.\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Cada ítem puede estar presente o no en un itemset. Para $ m $ ítems, hay $ 2^m $ combinaciones posibles (incluyendo el conjunto vacío). Restamos 1 para excluir el conjunto vacío, ya que un itemset debe contener al menos un ítem.\n",
        "\n",
        "---\n",
        "\n",
        "**Pregunta 2:** ¿Cuál es el principal desafío computacional al minar itemsets frecuentes en grandes conjuntos de datos?\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "El principal desafío es la explosión combinatoria del número de posibles itemsets, lo que conduce a un espacio de búsqueda exponencial. Esto resulta en un uso excesivo de memoria y tiempo de procesamiento, haciendo impráctico enumerar y evaluar todos los itemsets posibles.\n",
        "\n",
        "---\n",
        "\n",
        "**Pregunta 3:** Mencione al menos dos estrategias utilizadas por algoritmos como Apriori para manejar la complejidad computacional.\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "1. **Propiedad de Antimonotonía:** Si un itemset no es frecuente, ninguno de sus superconjuntos puede ser frecuente. Esto permite descartar muchos itemsets candidatos.\n",
        "2. **Generación de Candidatos Eficiente:** Apriori genera candidatos de tamaño $ k $ combinando itemsets frecuentes de tamaño $ k-1 $, reduciendo el número de itemsets que se evalúan.\n",
        "\n",
        "---\n",
        "\n",
        "**Pregunta 4:** ¿Por qué es impráctico utilizar un algoritmo ingenuo para minar itemsets frecuentes en conjuntos de datos reales?\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Porque el algoritmo ingenuo intenta generar y evaluar todos los itemsets posibles, lo cual es inviable debido al enorme número de combinaciones y las limitaciones de memoria y tiempo de procesamiento. Los conjuntos de datos reales suelen tener miles de ítems y millones de transacciones, lo que hace que este enfoque sea impráctico.\n",
        "\n",
        "---\n",
        "\n",
        "**Pregunta 5:** ¿Cómo afecta el umbral mínimo de soporte al número de itemsets frecuentes encontrados?\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Un umbral mínimo de soporte más alto reduce el número de itemsets frecuentes encontrados, ya que se requieren itemsets con mayor frecuencia para ser considerados frecuentes. Un umbral más bajo aumenta el número de itemsets frecuentes, incluyendo aquellos que aparecen con menor frecuencia, lo que puede resultar en un número excesivo de itemsets y ruido en los resultados.\n",
        "\n",
        "---\n",
        "\n",
        "## Resumen del Capítulo\n",
        "\n",
        "En este capítulo, hemos explorado en profundidad el problema de la minería de itemsets frecuentes:\n",
        "\n",
        "- **Definición del Problema**: Entendimos qué es la minería de itemsets frecuentes y su importancia en la extracción de patrones útiles en los datos.\n",
        "- **Complejidad Computacional**: Analizamos la explosión combinatoria del número de posibles itemsets y cómo esto afecta el procesamiento.\n",
        "- **Desafíos en Conjuntos de Datos Grandes**: Discutimos las limitaciones computacionales y la necesidad de algoritmos eficientes para manejar grandes volúmenes de datos.\n",
        "- **Análisis Teórico y Práctico con Python**: A través de ejemplos prácticos, demostramos las limitaciones de los enfoques ingenuos y la importancia de utilizar algoritmos optimizados.\n",
        "- **Quiz**: Evaluamos la comprensión de los conceptos clave mediante preguntas y respuestas.\n",
        "\n",
        "Este capítulo sienta las bases para introducir algoritmos avanzados como **Apriori** y **FP-Growth** en los próximos capítulos, que abordan estos desafíos y permiten minar itemsets frecuentes de manera eficiente en grandes conjuntos de datos.\n",
        "\n",
        "---\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- Agrawal, R., & Srikant, R. (1994). Fast algorithms for mining association rules. *Proceedings of the 20th International Conference on Very Large Data Bases*, 487-499.\n",
        "- Han, J., Pei, J., & Yin, Y. (2000). Mining frequent patterns without candidate generation. *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, 1-12.\n",
        "- Tan, P.-N., Steinbach, M., & Kumar, V. (2005). *Introduction to Data Mining*. Addison-Wesley.\n",
        "\n",
        "---\n",
        "\n",
        "## Próximos Pasos\n",
        "\n",
        "En el siguiente capítulo, comenzaremos a explorar el **Algoritmo Apriori**, uno de los algoritmos más influyentes en la minería de itemsets frecuentes. Aprenderemos cómo utiliza la propiedad de antimonotonía para reducir el espacio de búsqueda y cómo implementarlo en Python, tanto manualmente como utilizando bibliotecas especializadas.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "p7-0OBhtM9Fr"
      }
    }
  ]
}